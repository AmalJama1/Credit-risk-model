# -*- coding: utf-8 -*-
"""credit_risk_modelling

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/103QnsrOczSO93bFPaDDuAdwxlK12POJX

IMPORTING ALL NECCESSARY LIBRARIES
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix

"""LOAD DATASET

"""

credit_data_1 = pd.read_csv('/content/test_data.csv')
credit_data_2 = pd.read_csv('/content/train_data.csv')
credit_data_1.head(10)

credit_data_2.head(10)

"""*The dataset contains these variables:ID	Gender	Has a car	Has a property	Children count	Income	Employment status	Education level	Marital status	Dwelling	Age	Employment length	Has a mobile phone	Has a work phone	Has a phone	Has an email	Job title	Family member count	Account age	Is high risk*


*Both datasets are identical*

PRE-PROCESSING DATA

*Removing irrelevant features,handle missing values*
"""

# Checking for null values
credit_data_1.isnull()

#dropping null values in row
credit_data_1.dropna(inplace=True)

#dropping duplicates
credit_data_1.drop_duplicates(inplace=True)
print(credit_data_1)

#Data Manipulation
credit_data_1.describe

credit_data_1.info()

#Data Type Corrections
# Convert categorical variables to category dtype
categorical_columns = ['Gender', 'Has a car', 'Has a property', 'Employment status',
                       'Education level', 'Marital status', 'Dwelling', 'Job title']

for column in categorical_columns:
    credit_data_1[column] = credit_data_1[column].astype('category')

# Apply one-hot encoding to categorical variables
credit_data_1 = pd.get_dummies(credit_data_1, columns=categorical_columns, drop_first=True)

# Check updated data types and the presence of new columns
print("\nUpdated Data Types:\n", credit_data_1.dtypes)
print("\nFirst Few Rows of Updated DataFrame:\n", credit_data_1.head())

"""EXPLORATIVE DATASET ANALYSIS"""

# Visualization and Correlation Analysis

important_features = ['Income', 'Children count', 'Age', 'Employment length',
                      'Family member count', 'Account age']

# Subset the dataframe
correlation_data = credit_data_1[important_features]

# Calculate the correlation matrix
correlation_matrix = correlation_data.corr()

# Plotting the correlation matrix using a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".1f", linewidths=.5)
plt.title('Correlation Matrix of Key Variables')
plt.show()

# Check if 'Income' and 'Age' columns are present
if 'Income' in credit_data_1.columns and 'Age' in credit_data_1.columns:
    # Plotting distribution of Income
    plt.figure(figsize=(10, 6))
    sns.histplot(credit_data_1['Income'], bins=30, color='blue', kde=True)
    plt.title('Distribution of Income')
    plt.xlabel('Income')
    plt.ylabel('Frequency')
    plt.show()

    # Check if 'loan_status' column is present
    if 'Is high risk' in credit_data_1.columns:
        # Plotting Income by loan status (high risk vs low risk)
        high_risk_mask = credit_data_1['Is high risk'] == 1
        low_risk_mask = credit_data_1['Is high risk'] == 0

        plt.figure(figsize=(10, 6))
        sns.kdeplot(credit_data_1.loc[low_risk_mask, 'Income'], label='Low Risk', shade=True)
        sns.kdeplot(credit_data_1.loc[high_risk_mask, 'Income'], label='High Risk', shade=True)
        plt.title('Income by Loan Risk Status')
        plt.xlabel('Income')
        plt.ylabel('Density')
        plt.legend()
        plt.show()
    else:
        print("The 'Is high risk' column is not available in the dataset.")
else:
    print("Income or Age columns are not available in the dataset.")

if 'Is high risk' in credit_data_1.columns and 'Income' in credit_data_1.columns:
    # Create a boxplot for 'Income' by loan risk status
    try:
        plt.figure(figsize=(10, 6))
        sns.boxplot(x='Is high risk', y='Income', data=credit_data_1)
        plt.title('Boxplot of Income by Loan Risk Status')
        plt.xlabel('Loan Risk Status (0 = Low Risk, 1 = High Risk)')
        plt.ylabel('Income')
        plt.xticks(rotation=90)
        plt.show()
    except Exception as e:
        print(f"An error occurred during plotting: {e}")
else:
    print("The required columns 'Is high risk' or 'Income' are not available in the dataset.")

# Calculate summary statistics for 'Income' by loan risk status
income_stats_by_status = credit_data_1.groupby('Is high risk')['Income'].describe()
print(income_stats_by_status)

"""FEATURE SELECTION


"""

# Convert loan_status to a binary outcome for feature selection (1 for High Risk, 0 for Low Risk)
credit_data_1['loan_status_binary'] = credit_data_1['Is high risk'].apply(lambda x: 1 if x == 1 else 0)

# Selecting features for the model
features = credit_data_1.select_dtypes(include=[np.number]).columns.tolist()
features.remove('loan_status_binary')

# Prepare the features and target variable
X = credit_data_1[features]
y = credit_data_1['loan_status_binary']

# Create an imputer object with a median filling strategy
imputer = SimpleImputer(strategy='median')

# Apply imputer to our feature set: impute missing values with the median of each column
X_imputed = imputer.fit_transform(X)

# Fitting a Random Forest to assess feature importances
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_imputed, y)

# Get feature importances from the random forest model
feature_importances = rf.feature_importances_

# Create a DataFrame to view the features and their importances
features_importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': feature_importances
}).sort_values(by='Importance', ascending=False)

# Display the top 20 important features
top_20_features = features_importance_df.head(20)
print(top_20_features)

# Data splitting into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42)

# Scaling the features: StandardScaler is generally a good default scaler for many ML models
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Check the final size of the training and test sets
print(X_train_scaled.shape, X_test_scaled.shape, y_train.shape, y_test.shape)

"""MODELLING"""

# Initialize Logistic Regression
log_reg = LogisticRegression(max_iter=1000, random_state=42)
# Train Logistic Regression
log_reg.fit(X_train_scaled, y_train)
# Predict on testing data
y_pred_log_reg = log_reg.predict(X_test_scaled)
y_pred_proba_log_reg = log_reg.predict_proba(X_test_scaled)[:, 1]
accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)
precision_log_reg = precision_score(y_test, y_pred_log_reg)
recall_log_reg = recall_score(y_test, y_pred_log_reg)
f1_log_reg = f1_score(y_test, y_pred_log_reg)
print(accuracy_log_reg, precision_log_reg, recall_log_reg, f1_log_reg)

svm_classifier = SVC(probability=True, random_state=42)
svm_classifier.fit(X_train_scaled, y_train)
y_pred_svm = svm_classifier.predict(X_test_scaled)
y_pred_proba_svm = svm_classifier.predict_proba(X_test_scaled)[:, 1]
accuracy_svm = accuracy_score(y_test, y_pred_svm)
precision_svm = precision_score(y_test, y_pred_svm)
recall_svm = recall_score(y_test, y_pred_svm)
f1_svm = f1_score(y_test, y_pred_svm)
roc_auc_svm = roc_auc_score(y_test, y_pred_proba_svm)
print(accuracy_svm, precision_svm, recall_svm, f1_svm)

# Initialize k-Nearest Neighbors with k=5
knn_classifier = KNeighborsClassifier(n_neighbors=5)
knn_classifier.fit(X_train_scaled, y_train)
y_pred_knn = knn_classifier.predict(X_test_scaled)
y_pred_proba_knn = knn_classifier.predict_proba(X_test_scaled)[:, 1]
accuracy_knn = accuracy_score(y_test, y_pred_knn)
precision_knn = precision_score(y_test, y_pred_knn)
recall_knn = recall_score(y_test, y_pred_knn)
f1_knn = f1_score(y_test, y_pred_knn)
print(accuracy_knn, precision_knn, recall_knn, f1_knn)

# Initialize Decision Tree
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train_scaled, y_train)
y_pred_dt = dt_classifier.predict(X_test_scaled)
y_pred_proba_dt = dt_classifier.predict_proba(X_test_scaled)[:, 1]
accuracy_dt = accuracy_score(y_test, y_pred_dt)
precision_dt = precision_score(y_test, y_pred_dt)
recall_dt = recall_score(y_test, y_pred_dt)
f1_dt = f1_score(y_test, y_pred_dt)
print(accuracy_dt, precision_dt, recall_dt, f1_dt)

# Combine the metrics for all models into a single DataFrame
metrics_df = pd.DataFrame({
    'Model': ['Logistic Regression', 'SVM', 'k-NN', 'Decision Tree'],
    'Accuracy': [accuracy_log_reg, accuracy_svm,accuracy_knn, accuracy_dt],
    'Precision': [precision_log_reg, precision_svm,precision_knn, precision_dt],
    'Recall': [recall_log_reg, recall_svm,recall_knn, recall_dt],
    'F1-Score': [f1_log_reg, f1_svm, f1_knn, f1_dt],
})

# Setting up the plot aesthetics
sns.set(style="whitegrid")

# Plotting bar charts for each metric
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# Plot Accuracy
sns.barplot(x='Model', y='Accuracy', data=metrics_df, ax=axes[0, 0], palette='viridis')
axes[0, 0].set_title('Accuracy Comparison')
axes[0, 0].set_xlabel('Model')
axes[0, 0].set_ylabel('Accuracy')

# Plot Precision
sns.barplot(x='Model', y='Precision', data=metrics_df, ax=axes[0, 1], palette='viridis')
axes[0, 1].set_title('Precision Comparison')
axes[0, 1].set_xlabel('Model')
axes[0, 1].set_ylabel('Precision')

# Plot Recall
sns.barplot(x='Model', y='Recall', data=metrics_df, ax=axes[0, 2], palette='viridis')
axes[0, 2].set_title('Recall Comparison')
axes[0, 2].set_xlabel('Model')
axes[0, 2].set_ylabel('Recall')

# Plot F1-Score
sns.barplot(x='Model', y='F1-Score', data=metrics_df, ax=axes[1, 0], palette='viridis')
axes[1, 0].set_title('F1-Score Comparison')
axes[1, 0].set_xlabel('Model')
axes[1, 0].set_ylabel('F1-Score')

# Remove the empty subplot
fig.delaxes(axes[1, 2])

plt.tight_layout()
plt.show()

